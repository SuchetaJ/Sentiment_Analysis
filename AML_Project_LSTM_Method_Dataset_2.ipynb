{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feD4H1B0wz9k"
      },
      "source": [
        "Dataset: https://www.kaggle.com/datasets/jp797498e/twitter-entity-sentiment-analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvOagTXh25vg",
        "outputId": "ee57989c-1119-442e-cb8c-46f9aa51baef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in c:\\users\\sucheta jhunjhunwala\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.28.1)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\sucheta jhunjhunwala\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\sucheta jhunjhunwala\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (1.24.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in c:\\users\\sucheta jhunjhunwala\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\sucheta jhunjhunwala\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\sucheta jhunjhunwala\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\sucheta jhunjhunwala\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: requests in c:\\users\\sucheta jhunjhunwala\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\users\\sucheta jhunjhunwala\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: filelock in c:\\users\\sucheta jhunjhunwala\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\sucheta jhunjhunwala\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\sucheta jhunjhunwala\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: colorama in c:\\users\\sucheta jhunjhunwala\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.5)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\sucheta jhunjhunwala\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (1.26.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sucheta jhunjhunwala\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sucheta jhunjhunwala\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\sucheta jhunjhunwala\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2.0.12)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.0.1 -> 23.1.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TD0ihms5wuUO",
        "outputId": "f14b39f2-27d2-4921-a59f-ce488e861109"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to C:\\Users\\Sucheta\n",
            "[nltk_data]     Jhunjhunwala\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to C:\\Users\\Sucheta\n",
            "[nltk_data]     Jhunjhunwala\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "c:\\Users\\Sucheta Jhunjhunwala\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "# Import Libraries\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import zipfile\n",
        "from zipfile import ZipFile\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "stop_words=set(stopwords.words(\"english\"))\n",
        "\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "#from torchtext.vocab import Vocab\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "#from torchtext.data.utils import get_tokenizer\n",
        "#from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "text_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nSdARV460-or",
        "outputId": "208db74f-18b1-4277-d9f1-0fa3a4b7c90a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to C:\\Users\\Sucheta\n",
            "[nltk_data]     Jhunjhunwala\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to C:\\Users\\Sucheta\n",
            "[nltk_data]     Jhunjhunwala\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "#from torchtext.vocab import build_vocab_from_iterator\n",
        "from sklearn.metrics import accuracy_score\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "aJLbCGVkxQXW"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "XSGrJoMlwuWv"
      },
      "outputs": [],
      "source": [
        "# Set the path of the ZIP archive and the destination folder where the extracted files will be saved\n",
        "zip_file_path = 'archive.zip'\n",
        "destination_folder = '/content/drive/MyDrive/AML'\n",
        "\n",
        "# Open the ZIP archive using the ZipFile function and extract all its contents to the destination folder using the extractall method\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "kDV2VKJpwuZL",
        "outputId": "9511f551-f4d7-4ed4-8af2-dd828d60ed01"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Tweet_ID</th>\n",
              "      <th>Entity</th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>Tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2401</td>\n",
              "      <td>Borderlands</td>\n",
              "      <td>Positive</td>\n",
              "      <td>im getting on borderlands and i will murder yo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2401</td>\n",
              "      <td>Borderlands</td>\n",
              "      <td>Positive</td>\n",
              "      <td>I am coming to the borders and I will kill you...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2401</td>\n",
              "      <td>Borderlands</td>\n",
              "      <td>Positive</td>\n",
              "      <td>im getting on borderlands and i will kill you ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2401</td>\n",
              "      <td>Borderlands</td>\n",
              "      <td>Positive</td>\n",
              "      <td>im coming on borderlands and i will murder you...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2401</td>\n",
              "      <td>Borderlands</td>\n",
              "      <td>Positive</td>\n",
              "      <td>im getting on borderlands 2 and i will murder ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Tweet_ID       Entity Sentiment  \\\n",
              "0      2401  Borderlands  Positive   \n",
              "1      2401  Borderlands  Positive   \n",
              "2      2401  Borderlands  Positive   \n",
              "3      2401  Borderlands  Positive   \n",
              "4      2401  Borderlands  Positive   \n",
              "\n",
              "                                               Tweet  \n",
              "0  im getting on borderlands and i will murder yo...  \n",
              "1  I am coming to the borders and I will kill you...  \n",
              "2  im getting on borderlands and i will kill you ...  \n",
              "3  im coming on borderlands and i will murder you...  \n",
              "4  im getting on borderlands 2 and i will murder ...  "
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df=pd.read_csv(\"twitter_training.csv\",names=['Tweet_ID','Entity','Sentiment','Tweet'])\n",
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "3eQrMigzwub6",
        "outputId": "78133f3c-37e4-45f2-8a83-2781a1a30255"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Tweet_ID</th>\n",
              "      <th>Entity</th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>Tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3364</td>\n",
              "      <td>Facebook</td>\n",
              "      <td>Irrelevant</td>\n",
              "      <td>I mentioned on Facebook that I was struggling ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>352</td>\n",
              "      <td>Amazon</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>BBC News - Amazon boss Jeff Bezos rejects clai...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8312</td>\n",
              "      <td>Microsoft</td>\n",
              "      <td>Negative</td>\n",
              "      <td>@Microsoft Why do I pay for WORD when it funct...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4371</td>\n",
              "      <td>CS-GO</td>\n",
              "      <td>Negative</td>\n",
              "      <td>CSGO matchmaking is so full of closet hacking,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4433</td>\n",
              "      <td>Google</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Now the President is slapping Americans in the...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Tweet_ID     Entity   Sentiment  \\\n",
              "0      3364   Facebook  Irrelevant   \n",
              "1       352     Amazon     Neutral   \n",
              "2      8312  Microsoft    Negative   \n",
              "3      4371      CS-GO    Negative   \n",
              "4      4433     Google     Neutral   \n",
              "\n",
              "                                               Tweet  \n",
              "0  I mentioned on Facebook that I was struggling ...  \n",
              "1  BBC News - Amazon boss Jeff Bezos rejects clai...  \n",
              "2  @Microsoft Why do I pay for WORD when it funct...  \n",
              "3  CSGO matchmaking is so full of closet hacking,...  \n",
              "4  Now the President is slapping Americans in the...  "
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "val_df=pd.read_csv(\"twitter_validation.csv\",names=['Tweet_ID','Entity','Sentiment','Tweet'])\n",
        "val_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "keVE4T8qwueV"
      },
      "outputs": [],
      "source": [
        "# Preprocess the text data\n",
        "def preprocess_text(text):\n",
        "    text = str(text).lower()  # Convert the text to a string\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "    text = re.sub(r'\\W+', ' ', text)\n",
        "\n",
        "    tokens = word_tokenize(text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    return ' '.join(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "U3rM0ke71FdY"
      },
      "outputs": [],
      "source": [
        "train_df['Tweet'] = train_df['Tweet'].apply(preprocess_text)\n",
        "val_df['Tweet'] = val_df['Tweet'].apply(preprocess_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "DQbxIintC1ps"
      },
      "outputs": [],
      "source": [
        "sentiment_mapping = {'Positive': 0, 'Negative': 1, 'Neutral': 2, 'Irrelevant': 3}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "SSdRq_vPwuiw"
      },
      "outputs": [],
      "source": [
        "class TextDataset(Dataset):\n",
        "    def __init__(self, tweets, entities, sentiments, tokenizer, max_len):\n",
        "        self.tweets = tweets\n",
        "        self.entities = entities\n",
        "        self.sentiments = [sentiment_mapping[sentiment] for sentiment in sentiments]  # Update this line\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tweets)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        tweet = str(self.tweets[item])\n",
        "        entity = str(self.entities[item])\n",
        "        sentiment = self.sentiments[item]\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            tweet,\n",
        "            entity,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "            return_overflowing_tokens=True,  # Add this line\n",
        "        )\n",
        "\n",
        "        # Handle overflowing tokens\n",
        "        if 'overflowing_tokens' in encoding:\n",
        "            encoding['input_ids'] = encoding['input_ids'][:, :self.max_len]\n",
        "            encoding['attention_mask'] = encoding['attention_mask'][:, :self.max_len]\n",
        "\n",
        "        return {\n",
        "            'tweet': tweet,\n",
        "            'entity': entity,\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'sentiments': torch.tensor(sentiment, dtype=torch.long),\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "5m3EshZgJnQ-"
      },
      "outputs": [],
      "source": [
        "class TextDataset(Dataset):\n",
        "    def __init__(self, tweets, entities, sentiments, tokenizer, max_len):\n",
        "        self.tweets = tweets\n",
        "        self.entities = entities\n",
        "        self.sentiments = [sentiment_mapping[sentiment] for sentiment in sentiments]\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tweets)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        tweet = str(self.tweets[item])\n",
        "        entity = str(self.entities[item])\n",
        "        sentiment = self.sentiments[item]\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            tweet,\n",
        "            entity,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            truncation='only_first',  # Update this line\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'tweet': tweet,\n",
        "            'entity': entity,\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'sentiments': torch.tensor(sentiment, dtype=torch.long),\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "kgiNc8NQwurI"
      },
      "outputs": [],
      "source": [
        "# Create LSTM model\n",
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, n_classes, dropout=0.3):\n",
        "        super(LSTMClassifier, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, n_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        embedded = self.dropout(self.embedding(input_ids))\n",
        "        packed_output, (hidden, cell) = self.lstm(embedded)\n",
        "        hidden = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)\n",
        "        output = self.fc(hidden)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "XZxN-GRi1zAc"
      },
      "outputs": [],
      "source": [
        "# Training and validation\n",
        "def train_epoch(model, data_loader, optimizer, device):\n",
        "    model.train()\n",
        "    for d in tqdm(data_loader):\n",
        "        input_ids = d['input_ids'].to(device)\n",
        "        attention_mask = d['attention_mask'].to(device)\n",
        "        sentiments = d['sentiments'].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "        loss = nn.CrossEntropyLoss()(outputs, sentiments)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "def evaluate(model, data_loader, device):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for d in tqdm(data_loader):\n",
        "            input_ids = d['input_ids'].to(device)\n",
        "            attention_mask = d['attention_mask'].to(device)\n",
        "            sentiments = d['sentiments'].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "            predictions.extend(preds.cpu())\n",
        "            true_labels.extend(sentiments.cpu())\n",
        "\n",
        "    return accuracy_score(true_labels, predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "yrK6s7II1zDT"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 10\n",
        "LEARNING_RATE = 1e-3\n",
        "MAX_LEN = 256\n",
        "HIDDEN_DIM = 128\n",
        "EMBED_DIM = 128\n",
        "DROPOUT = 0.3\n",
        "N_CLASSES = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "7dRmr4sg1zFU"
      },
      "outputs": [],
      "source": [
        "# Create DataLoaders\n",
        "train_dataset = TextDataset(train_df['Tweet'], train_df['Entity'], train_df['Sentiment'], text_tokenizer, MAX_LEN)\n",
        "val_dataset = TextDataset(val_df['Tweet'], val_df['Entity'], val_df['Sentiment'], text_tokenizer, MAX_LEN)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "E3A9BtNE1zHr"
      },
      "outputs": [],
      "source": [
        "# Initialize the model, optimizer, and device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = LSTMClassifier(len(text_tokenizer), EMBED_DIM, HIDDEN_DIM, N_CLASSES, DROPOUT).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "errsPIE5DTAX",
        "outputId": "74cc3730-2760-40db-b121-84a4a66296f7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFgYTzWcQE9v",
        "outputId": "747e55ac-e499-4e53-a008-6c967ef7f40c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2334/2334 [13:49<00:00,  2.81it/s] \n",
            "100%|██████████| 32/32 [00:05<00:00,  5.53it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation accuracy: 0.6980\n",
            "Epoch: 2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2334/2334 [13:36<00:00,  2.86it/s]\n",
            "100%|██████████| 32/32 [00:02<00:00, 12.43it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation accuracy: 0.8540\n",
            "Epoch: 3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2334/2334 [08:37<00:00,  4.51it/s]\n",
            "100%|██████████| 32/32 [00:02<00:00, 11.40it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation accuracy: 0.8890\n",
            "Epoch: 4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2334/2334 [07:51<00:00,  4.95it/s]\n",
            "100%|██████████| 32/32 [00:02<00:00, 13.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation accuracy: 0.9260\n",
            "Epoch: 5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2334/2334 [08:10<00:00,  4.75it/s]\n",
            "100%|██████████| 32/32 [00:02<00:00, 13.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation accuracy: 0.9440\n",
            "Epoch: 6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2334/2334 [07:46<00:00,  5.00it/s]\n",
            "100%|██████████| 32/32 [00:02<00:00, 13.37it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation accuracy: 0.9560\n",
            "Epoch: 7\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2334/2334 [07:49<00:00,  4.97it/s]\n",
            "100%|██████████| 32/32 [00:02<00:00, 12.47it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation accuracy: 0.9530\n",
            "Epoch: 8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2334/2334 [08:07<00:00,  4.79it/s]\n",
            "100%|██████████| 32/32 [00:03<00:00,  9.80it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation accuracy: 0.9640\n",
            "Epoch: 9\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2334/2334 [09:22<00:00,  4.15it/s]\n",
            "100%|██████████| 32/32 [00:02<00:00, 11.15it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation accuracy: 0.9640\n",
            "Epoch: 10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2334/2334 [09:34<00:00,  4.07it/s]\n",
            "100%|██████████| 32/32 [00:02<00:00, 12.05it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation accuracy: 0.9690\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Train and validate the model\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"Epoch: {epoch+1}\")\n",
        "    train_epoch(model, train_loader, optimizer, device)\n",
        "    accuracy = evaluate(model, val_loader, device)\n",
        "    print(f\"Validation accuracy: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "RVu42Zko1zMg"
      },
      "outputs": [],
      "source": [
        "# Save the model\n",
        "model_path = \"sentiment_LSTM_model.pth\"\n",
        "torch.save(model.state_dict(), model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qf0nFeHz1zPR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ii6akKOhwut3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBTz4O_vwuw2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O46Lly21wuzm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CnyJ-70mwu5W"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5SsnBmNCwu8G"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YIP6nckxwu_J"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39i3r7dwwvB0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8Q2t58hwvE3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ksuG8cEOwvH0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWCatJVfwvKi"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
